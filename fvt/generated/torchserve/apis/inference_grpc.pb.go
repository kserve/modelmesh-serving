// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
// versions:
// - protoc-gen-go-grpc v1.2.0
// - protoc             v3.21.12
// source: inference.proto

package torchserveapi

import (
	context "context"
	grpc "google.golang.org/grpc"
	codes "google.golang.org/grpc/codes"
	status "google.golang.org/grpc/status"
	emptypb "google.golang.org/protobuf/types/known/emptypb"
)

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
// Requires gRPC-Go v1.32.0 or later.
const _ = grpc.SupportPackageIsVersion7

// InferenceAPIsServiceClient is the client API for InferenceAPIsService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream.
type InferenceAPIsServiceClient interface {
	// Check health status of the TorchServe server.
	Ping(ctx context.Context, in *emptypb.Empty, opts ...grpc.CallOption) (*TorchServeHealthResponse, error)
	// Predictions entry point to get inference using default model version.
	Predictions(ctx context.Context, in *PredictionsRequest, opts ...grpc.CallOption) (*PredictionResponse, error)
}

type inferenceAPIsServiceClient struct {
	cc grpc.ClientConnInterface
}

func NewInferenceAPIsServiceClient(cc grpc.ClientConnInterface) InferenceAPIsServiceClient {
	return &inferenceAPIsServiceClient{cc}
}

func (c *inferenceAPIsServiceClient) Ping(ctx context.Context, in *emptypb.Empty, opts ...grpc.CallOption) (*TorchServeHealthResponse, error) {
	out := new(TorchServeHealthResponse)
	err := c.cc.Invoke(ctx, "/org.pytorch.serve.grpc.inference.InferenceAPIsService/Ping", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *inferenceAPIsServiceClient) Predictions(ctx context.Context, in *PredictionsRequest, opts ...grpc.CallOption) (*PredictionResponse, error) {
	out := new(PredictionResponse)
	err := c.cc.Invoke(ctx, "/org.pytorch.serve.grpc.inference.InferenceAPIsService/Predictions", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// InferenceAPIsServiceServer is the server API for InferenceAPIsService service.
// All implementations must embed UnimplementedInferenceAPIsServiceServer
// for forward compatibility
type InferenceAPIsServiceServer interface {
	// Check health status of the TorchServe server.
	Ping(context.Context, *emptypb.Empty) (*TorchServeHealthResponse, error)
	// Predictions entry point to get inference using default model version.
	Predictions(context.Context, *PredictionsRequest) (*PredictionResponse, error)
	mustEmbedUnimplementedInferenceAPIsServiceServer()
}

// UnimplementedInferenceAPIsServiceServer must be embedded to have forward compatible implementations.
type UnimplementedInferenceAPIsServiceServer struct {
}

func (UnimplementedInferenceAPIsServiceServer) Ping(context.Context, *emptypb.Empty) (*TorchServeHealthResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Ping not implemented")
}
func (UnimplementedInferenceAPIsServiceServer) Predictions(context.Context, *PredictionsRequest) (*PredictionResponse, error) {
	return nil, status.Errorf(codes.Unimplemented, "method Predictions not implemented")
}
func (UnimplementedInferenceAPIsServiceServer) mustEmbedUnimplementedInferenceAPIsServiceServer() {}

// UnsafeInferenceAPIsServiceServer may be embedded to opt out of forward compatibility for this service.
// Use of this interface is not recommended, as added methods to InferenceAPIsServiceServer will
// result in compilation errors.
type UnsafeInferenceAPIsServiceServer interface {
	mustEmbedUnimplementedInferenceAPIsServiceServer()
}

func RegisterInferenceAPIsServiceServer(s grpc.ServiceRegistrar, srv InferenceAPIsServiceServer) {
	s.RegisterService(&InferenceAPIsService_ServiceDesc, srv)
}

func _InferenceAPIsService_Ping_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(emptypb.Empty)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceAPIsServiceServer).Ping(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/org.pytorch.serve.grpc.inference.InferenceAPIsService/Ping",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceAPIsServiceServer).Ping(ctx, req.(*emptypb.Empty))
	}
	return interceptor(ctx, in, info, handler)
}

func _InferenceAPIsService_Predictions_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(PredictionsRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(InferenceAPIsServiceServer).Predictions(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/org.pytorch.serve.grpc.inference.InferenceAPIsService/Predictions",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(InferenceAPIsServiceServer).Predictions(ctx, req.(*PredictionsRequest))
	}
	return interceptor(ctx, in, info, handler)
}

// InferenceAPIsService_ServiceDesc is the grpc.ServiceDesc for InferenceAPIsService service.
// It's only intended for direct use with grpc.RegisterService,
// and not to be introspected or modified (even as a copy)
var InferenceAPIsService_ServiceDesc = grpc.ServiceDesc{
	ServiceName: "org.pytorch.serve.grpc.inference.InferenceAPIsService",
	HandlerType: (*InferenceAPIsServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Ping",
			Handler:    _InferenceAPIsService_Ping_Handler,
		},
		{
			MethodName: "Predictions",
			Handler:    _InferenceAPIsService_Predictions_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "inference.proto",
}
